from pdfminer.high_level import extract_text
import spacy
import re
from pathlib import Path
import pandas as pd
import sys
import warnings
import json
from typing import Dict, List, Any, Optional

# Suppress minor warnings often generated by external libraries
warnings.filterwarnings('ignore')

# --- CONFIGURATION ---
# Define the directory path where all your resume PDFs are located
RESUMES_DIR = Path(r"../../data/resumes/banking")

# Output files
PARSED_CSV_FILE = "../../data/ner/parsed_resumes_dataframe.csv"
DOCANNO_JSONL_FILE = "../../data/ner/resume_test_data.jsonl"

# Limit how many PDF files to process (set None for ALL files)
MAX_FILES_TO_PROCESS = 5
# ---------------------

# Global spaCy NLP object
nlp = None 
KNOWN_SKILLS = [
    'Python', 'SQL', 'Java', 'R', 'C++', 'TensorFlow', 'Keras', 'AWS', 
    'Azure', 'GCP', 'Machine Learning', 'Data Analysis', 'NLP', 'DSA', 
    'Tableau', 'Power BI', 'Excel', 'Oracle'
]

# --------------------------------------------------------
# STEP 1: INITIALIZATION
# --------------------------------------------------------

def initialize_spacy():
    """Initializes and loads the spaCy model."""
    global nlp
    try:
        print("--- 1. Attempting to load spaCy English model (en_core_web_sm) ---")
        nlp = spacy.load("en_core_web_sm")
        print("✅ spaCy model loaded successfully.")
        return True
    except OSError:
        print("\n⚠️ MODEL NOT FOUND: The 'en_core_web_sm' model needs to be downloaded.")
        print("Please run the command: python -m spacy download en_core_web_sm")
        sys.exit(1)
    except Exception as e:
        print(f"\n❌ FATAL ERROR: Could not load spaCy model. Details: {e}")
        sys.exit(1)

# --------------------------------------------------------
# STEP 2: CORE EXTRACTION & PARSING FUNCTIONS
# --------------------------------------------------------

def extract_text_from_pdf(pdf_path: Path) -> Optional[str]:
    """Extracts text from a single PDF file and cleans it."""
    try:
        text = extract_text(pdf_path)
        # Clean up excessive whitespace/newlines
        clean_text = re.sub(r'\s+', ' ', text).strip()
        return clean_text
    except Exception:
        # Gracefully handle corrupted or encrypted PDFs
        return None

def extract_entities(resume_text: str) -> Dict[str, Any]:
    """
    Performs basic NER (Name, Org) and uses Regex (Email, Skills) 
    for key resume fields.
    """
    if not resume_text or not nlp:
        return {}

    # Apply the loaded spaCy model to the text
    doc = nlp(resume_text)
    data: Dict[str, Any] = {}
    
    # --- Entity Extraction using spaCy ---
    data['Name'] = next((ent.text for ent in doc.ents if ent.label_ == "PERSON"), "N/A")
    data['Organizations'] = ", ".join(list(set(ent.text for ent in doc.ents if ent.label_ == "ORG")))
    
    # --- Extraction using Regular Expressions ---
    email_match = re.search(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', resume_text)
    data['Email'] = email_match.group(0) if email_match else "N/A"
    
    # Simple Skills Extraction
    found_skills = set()
    for skill in KNOWN_SKILLS:
        if re.search(r'\b' + re.escape(skill) + r'\b', resume_text, re.IGNORECASE):
            found_skills.add(skill)
            
    data['Skills'] = ", ".join(list(found_skills))
    data['Full_Text'] = resume_text 
    
    return data

# --------------------------------------------------------
# STEP 3: DOCANNO PREPARATION FUNCTION
# --------------------------------------------------------

def prepare_data_for_doccano(data: List[Dict[str, Any]], output_jsonl_path: Path):
    """Converts parsed data into JSONL format for Doccano."""
    print(f"\n--- 3. Preparing {len(data)} documents for Doccano ---")
    doccano_data = []
    
    for doc_data in data:
        if doc_data.get('Full_Text') and doc_data['Full_Text'] != 'N/A':
            document = {
                "text": str(doc_data['Full_Text']),
                "meta": {"file_name": doc_data.get('File_Name', 'N/A')}
            }
            doccano_data.append(document)

    with open(output_jsonl_path, 'w', encoding='utf-8') as f:
        for doc in doccano_data:
            f.write(json.dumps(doc, ensure_ascii=False) + '\n')

    print(f"✅ Doccano preparation complete. Output saved to: {output_jsonl_path}")


# --------------------------------------------------------
# MAIN EXECUTION
# --------------------------------------------------------
if __name__ == "__main__":
    
    # Check directory and load spaCy model
    if not RESUMES_DIR.is_dir():
        print(f"❌ Error: Directory not found at {RESUMES_DIR}")
        sys.exit(1)

    if not initialize_spacy():
        sys.exit(1)
            
    all_resumes_data: List[Dict[str, Any]] = []
    pdf_files = list(RESUMES_DIR.glob("*.pdf"))

    # Apply file limit from config
    if MAX_FILES_TO_PROCESS is not None:
        pdf_files = pdf_files[:MAX_FILES_TO_PROCESS]
    
    print("\n" + "="*70)
    print(f"STARTING BATCH PROCESSING: {RESUMES_DIR.name} (Processing {len(pdf_files)} PDFs)")
    print("="*70)
    
    if not pdf_files:
        print("No PDF files found. Process stopped.")
        sys.exit(0)

    # --- Task 1 & 2: Extract Text and Parse Entities ---
    for i, pdf_path in enumerate(pdf_files, 1):
        file_name = pdf_path.name
        print(f"\n[{i}/{len(pdf_files)}] Processing: {file_name}")
        
        resume_text = extract_text_from_pdf(pdf_path)
        
        if resume_text:
            extracted_info = extract_entities(resume_text)
            extracted_info['File_Name'] = file_name
            all_resumes_data.append(extracted_info)
            
            print(f"    -> Extracted Name: {extracted_info.get('Name', 'N/A')}")
        else:
            print("    -> FAILED to extract text. Skipping NER.")
            all_resumes_data.append({
                'File_Name': file_name, 
                'Name': 'FAILED TO PARSE', 
                'Full_Text': 'N/A',
                'Email': 'N/A', 
                'Skills': 'N/A', 
                'Organizations': 'N/A'
            })

    # 3. Save results
    if all_resumes_data:
        df = pd.DataFrame(all_resumes_data)
        cols = ['File_Name', 'Name', 'Email', 'Skills', 'Organizations', 'Full_Text']
        df = df[cols]
        
        df.to_csv(PARSED_CSV_FILE, index=False)
        
        print("\n" + "="*70)
        print("✅ BATCH PARSING COMPLETE.")
        print(f"Summary DataFrame saved to: {PARSED_CSV_FILE}")
        print("="*70)

        # Prepare data for Doccano
        prepare_data_for_doccano(all_resumes_data, DOCANNO_JSONL_FILE)
